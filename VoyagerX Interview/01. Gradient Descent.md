# Gradient Descent

- 일반적으로 풀이하면 **"1차 미분계수를 이용하여 함수의 최솟값을 찾아가는 최적화 알고리즘이다"** 라고 할 수 있다.
- 이를 딥러닝 학습에서 풀이한다면, **"Cost Function이 최솟값이 될 때의 파라미터를 찾는 Iterative Method 최적화 알고리즘이다"** 라고 할 수 있다.
- 대표적인 설명방법으로 **"눈을 감고 산을 내려가는데 가장 낮은 방향으로 한 걸음씩 움직이는 상황"** 비유되기도 한다.
- **결국 목표는 cost function의 Global Minimum을 찾는 것이다.**

# Gradient Descent의 동작과정

1. 현재 구현된 모델에서의 Cost Function을 확인한다.
2. Cost Function의 Gradient를 1차 미분계수를 이용하여 계산한다.
3. 이때 Gradient는 현재 Cost Function에서의 기울기 정도를 의미한다. 즉, 값이 클수록 크게 기울어져 있다고 생각한다.
4. Learning rate만큼 곱하여 **Cost Function 값이 작아지는 방향**으로 움직인다. 움직인다는 것은 파라미터를 조정한다는 의미이다.
5. 위 2~4과정을 종료조건에 만족할 만큼 반복한다.

# 종료조건

- 반복 횟수를 설정하여 종료한다.
- Gradient가 일정 값 이하로 계산되면 종료한다. (norm을 이용한다. 파라미터가 복수이면 Gradient는 벡터로 나온다.)

# 종류

1. Batch Gradient Descent
2. Stochastic Gradient Descent
3. mini-batch Gradient Descent (SGD와 혼용하여 사용하기도 함)
4. Momentum (관성 추가)
5. RMSProp (보폭 조절)
6. Adam (관성 & 보폭 조절)

![img](https://blog.kakaocdn.net/dn/ot87x/btq5Nb9BcXp/0zglSeGqmV8qam2fMZDn81/img.jpg)

# Batch Gradient Descent

- **"모든 학습 데이터를 사용하여 Cost Function의 Global Minimum을 찾는 방법"** 라는 컨셉으로 한다.
- 장점 : 모든 데이터를 사용하기 때문에 SGD, Mini-Batch 보단 수렴성이 안정적이다.
- 단점 : 모든 데이터를 사용하기 때문에 훈련 데이터의 크기가 커지면 계산 시간이 오래 걸린다. 또한 Learning rate에 따라 수렴 시간과 수렴 여부가 크게 달라진다.
- 위와 같은 문제 때문에 **SGD, mini-batch** 같은 방법들이 나왔다.

# SGD, mini-batch

- SGD는 **하나의 샘플**을 이용하여 업데이트하는 방법이다.
- mini-batch는 **batch size 만큼 데이터를 쪼개어** 업데이트하는 방법이다.
- 두 방법은 샘플 데이터를 얼마나 사용하냐에 따라 다르지만, 보통 SGD로 혼용해서 사용하기도 한다.
- 장점 : 계산시간이 짧다.
- 단점 : 수렴성이 변칙적이다. (Shooting, 단편적인 데이터만을 이용하여 업데이트) Local Minimum 문제가 있다. batch size를 결정해야 한다.
- 이와 같은 문제를 해결하기 위해 **Momentum, RMSProp** 와 같은 방법들이 나왔다.

# Momentum

- 이전 Gradient 정보까지 이용하여 파라미터를 업데이트한다.
- 즉, Momentum은 이전까지 계산한 gradient를 가져와 같이 업데이트한다.
- 식은 다음과 같이 사용한다.
  - $V_t = m * V_{t-1} - r*\nabla_w J(w_t)$
  - $w_{t+1} = w_t + V_t$
  - $V_1 = -r*\nabla_wJ(w_1), \ \  \ V_2 = m*V_1-r*\nabla_wJ(w_2) , \ \ ...$
  - 즉, V는 이전까지의 업데이트 정보를 계산하여 가지고 이를 반영하여 파라미터를 업데이트한다.

# RMSProp

- 이전까지는 Learning rate가 고정되어 학습 속도와 보폭 조절이 어려운 문제점이 있었다. 그래서 학습할수록 Learning rate를 줄여 나가며 학습하는 방법을 제안하였다.**(Learning Rate Decay)** 일반적으로 초기에는 보폭을 넙게 그리고 후반에는 보폭을 좁게해야 Global Minimum에 수렴할 가능성이 높아진다.
- 지수이동평균을 적용하여 과거일수록 gradient는 적게반영 후 learning rate를 조절한다.
- 식은 다음과 같이 사용한다.
  - $G_t = \gamma * G_{t-1} + (1 - \gamma) * (\nabla_wJ(w_t))^2$
  - $w_{t+1} = w_t - \frac{r}{\sqrt {G_t+e}}*\nabla_wJ(w_t)$
  - $G_1 = (1-\gamma)*(\nabla_wJ(w_1))^2, \ \ \ \ G_2 = \gamma * G_1+ (1-\gamma)*(\nabla_wJ(w_2))^2, \ \ \ ...$
  - 즉, G는 지수이동평균으로 최신 기울기를 더 반영하여 Learning rate를 줄여준다. 지수이동평균을 사용한 이유는 AdaGrad 방법에서 사용하지 않았을때 값이 무한대로 커져 Learning rate가 사라지는 현상이 발생하였기 때문이다.

# Adam

- Momentum과 RMSProp를 합친 방법이다.

![캡처](../images/캡처.PNG)

- m은 momentum 항을 의미하고, v는 Learning rate decay를 의미한다.

# BackPropagation

- 단순히 neural network에서 gradient descent를 chain rule을 사용하여 단순화 시킨 것

# Cost Function의 종류

- 다음과 같이 나눌 수 있고, 각각의 장단점은 이렇게 존재한다.
- 표로 정리하면 더 좋고
- Regression
  - ME (Mean Error)
    - 수식: $\frac{1}{n}\sum_{i=0}^{n}(y-\hat{y})$
    - 단점: error값의 범위가 음수, 양수 가리지 않고 모두 더하므로 실제 오차보다 작게 나올 수 있다.
  - MAE (Mean Absolute Error)
    - 수식: $\frac{1}{n}\sum_{i=0}^{n}|y-\hat{y}|$
    - 장점: MSE 보다 이상치의 영향을 적게 받는다.
    - 단점: 미분 불가능한 지점이 존재한다.
  - **MSE (Mean Squared Error)**
    - 수식: $\frac{1}{n}\sum_{i=0}^{n}(y-\hat{y})^2$
    - 장점: 모든 구간에서 미분 가능하다.
    - 단점: MAE 보다 이상치의 영향을 받는다.
  - RMSE (Root Mean Squared Error)
    - 수식: $\sqrt{\frac{1}{n}\sum_{i=0}^{n}(y-\hat{y})^2}$
    - 장점: MSE보다 이상치의 영향을 덜 받는다.
    - 단점: 미분 불가?, MAE와 MSE의 중간 포지션
  - MSLE (Mean Squared Log Error)
  - MAD (Median Absolute Deviation)

- Classification
  - **Cross Entropy**
    - 수식: $-\sum_{i=0}^{C}y_i*log(\hat{y})$
