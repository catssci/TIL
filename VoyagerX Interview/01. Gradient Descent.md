# Gradient Descent

- 일반적으로 풀이하면 **"1차 미분계수를 이용하여 함수의 최솟값을 찾아가는 반복 알고리즘이다"** 라고 할 수 있다.
- 이를 딥러닝 학습에서 풀이한다면, **"Cost Function이 최솟값이 될 때의 파라미터를 찾는 알고리즘이다"** 라고 할 수 있다.
- 대표적인 설명방법으로 **"눈을 감고 산을 내려가는데 가장 낮은 방향으로 한 걸음씩 움직이는 상황"** 비유되기도 한다.

# Gradient Descent의 동작과정

1. 현재 구현된 모델에서의 Cost Function을 확인한다.
2. Cost Function의 Gradient를 1차 미분계수를 이용하여 계산한다.
3. 이때 Gradient는 현재 Cost Function에서의 기울기 정도를 의미한다. 즉, 값이 클수록 크게 기울어져 있다고 생각한다.
4. Learning rate만큼 곱하여 **내려가는 방향**으로 움직인다. 움직인다는 것은 파라미터를 조정한다는 의미이다.
5. 위 2~4과정을 종료조건에 만족할 만큼 반복한다.

# 종료조건

- 반복 횟수를 설정하여 종료한다.
- Gradient가 일정 값 이하로 계산되면 종료한다. (norm을 이용한다. 파라미터가 복수이면 Gradient는 벡터로 나온다.)

# 종류

1. Batch Gradient Descent
2. Stochastic Gradient Descent
3. mini-batch Gradient Descent (SGD와 혼용하여 사용하기도 함)
4. Momentum (관성 추가)
5. RMSProp (보폭 조절)
6. Adam (관성 & 보폭 조절)

![img](https://blog.kakaocdn.net/dn/ot87x/btq5Nb9BcXp/0zglSeGqmV8qam2fMZDn81/img.jpg)

# Batch Gradient Descent

- 가장 기본적인 방법으로 **"모든 학습 데이터를 사용하여 Cost Function의 Global Minimum을 찾는 방법"** 라는 컨셉으로 한다.
- 장점 : 모든 데이터를 사용하기 때문에 SGD, Mini-Batch 보단 수렴성이 안정적이다.
- 단점 : 모든 데이터를 사용하기 때문에 훈련 데이터의 크기가 커지면 계산 시간이 오래 걸린다. 또한 Learning rate에 따라 수렴 시간과 수렴 여부가 크게 달라진다.
- 위와 같은 문제 때문에 **SGD, mini-batch** 같은 방법들이 나왔다.

# SGD, mini-batch

- SGD는 **하나의 샘플**을 이용하여 업데이트하는 방법이다.
- mini-batch는 **batch size 만큼 데이터를 쪼개어** 업데이트하는 방법이다.
- 두 방법은 샘플 데이터를 얼마나 사용하냐에 따라 다르지만, 보통 SGD로 혼용해서 사용하기도 한다.
- 장점 : 계산시간이 짧다.
- 단점 : 수렴성이 변칙적이다. (Shooting, 단편적인 데이터만을 이용하여 업데이트) Local Minimum 문제가 있다. batch size를 결정해야 한다.
- 이와 같은 문제를 해결하기 위해 **Momentum, RMSProp** 와 같은 방법들이 나왔다.

# Momentum

- 이전 Gradient 정보까지 이용하여 파라미터를 업데이트한다.
- 즉, Momentum은 이전까지 계산한 gradient를 가져와 같이 업데이트한다.
- 식은 다음과 같이 사용한다.
  - $V_t = m * V_{t-1} - r*\nabla_w J(w_t)$
  - $w_{t+1} = w_t + V_t$
  - $V_1 = -r*\nabla_wJ(w_1), \ \  \ V_2 = m*V_1-r*\nabla_wJ(w_2) , \ \ ...$
  - 즉, V는 이전까지의 업데이트 정보를 계산하여 가지고 이를 반영하여 파라미터를 업데이트한다.

# RMSProp

- 학습할수록 Learning rate를 줄여 나가며 학습하는 방법이다. **(Learning Rate Decay)**
- 지수이동평균을 적용하여 과거일수록 gradient는 적게반영 후 learning rate를 조절한다.
- 식은 다음과 같이 사용한다.
  - $G_t = \gamma * G_{t-1} + (1 - \gamma) * (\nabla_wJ(w_t))^2$
  - $w_{t+1} = w_t - \frac{r}{\sqrt {G_t+e}}*\nabla_wJ(w_t)$
  - $G_1 = (1-\gamma)*(\nabla_wJ(w_1))^2, \ \ \ \ G_2 = \gamma * G_1+ (1-\gamma)*(\nabla_wJ(w_2))^2, \ \ \ ...$
  - 즉, G는 지수이동평균으로 최신 기울기를 더 반영하여 Learning rate를 줄여준다. 지수이동평균을 사용한 이유는 AdaGrad 방법에서 사용하지 않았을때 값이 무한대로 커져 Learning rate가 사라지는 현상이 발생하였기 때문이다.

# Adam

- Momentum과 RMSProp를 합친 방법이다.

