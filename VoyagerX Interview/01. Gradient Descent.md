# Gradient Descent

- 일반적으로 풀이하면 **"1차 미분계수를 이용하여 함수의 최솟값을 찾아가는 반복 알고리즘이다"** 라고 할 수 있다.
- 이를 딥러닝 학습에서 풀이한다면, **"Cost Function이 최솟값이 될 때의 파라미터를 찾는 알고리즘이다"** 라고 할 수 있다.

# Gradient Descent의 동작과정

1. 현재 구현된 모델에서의 Cost Function을 확인한다.
2. Cost Function의 Gradient를 1차 미분계수를 이용하여 계산한다.
3. 이때 Gradient는 현재 Cost Function에서의 기울기 정도를 의미한다. 즉, 값이 클수록 크게 기울어져 있다고 생각한다.
4. Learning rate만큼 곱하여 움직인다. 움직인다는 것은 파라미터를 조정한다는 의미이다.
5. 위 과정을 종료조건에 만족할 만큼 반복한다.

# 종료조건

- 반복 횟수를 설정하여 종료한다.
- Gradient가 일정 값 이하로 나타나면 종료한다. (norm을 이용한다. 파라미터가 복수이면 Gradient는 벡터로 나온다.)

# 종류

1. Batch Gradient Descent
2. Stochastic Gradient Descent
3. mini-batch Gradient Descent (SGD와 혼용하여 사용하기도 함)
4. Momentum (관성 추가)
5. RMSProp (보폭 조절)
6. Adam (관성 & 보폭 조절)

![img](https://blog.kakaocdn.net/dn/ot87x/btq5Nb9BcXp/0zglSeGqmV8qam2fMZDn81/img.jpg)

# Batch Gradient Descent

- 가장 기본적인 방법으로 **"모든 학습 데이터를 사용하여 파라미터를 업데이트한다."** 라는 컨셉으로 한다.
- 해당 방법은 가장 심플한 방법이다.
- 그러나 이와 같은 방법은 데이터가 큰 상황에서 실행하기 힘들 수 있다.
- 그래서 **SGD, mini-batch** 같은 방법들이 나왔다.

# SGD, mini-batch

- SGD는 하나의 샘플을 이용하여 업데이트하는 방법이다.
- mini-batch는 batch 사이즈를 정하여 학습 데이터를 쪼개어 업데이트하는 방법이다.
- 두 방법은 샘플 데이터를 얼마나 사용하냐에 따라 다르지만, 보통 SGD로 혼용해서 사용하기도 한다.
- 그러나 수렴이 불안정하고 (**Local Minimum 수렴**), batch size를 결정해야하는 단점이 있다.
- 이와 같은 문제를 해결하기 위해 **Momentum, RMSProp** 와 같은 방법들이 나왔다.

# Momentum

- 이전 Gradient 정보까지 이용하여 파라미터를 업데이트한다.
- 즉, Momentum은 이전까지 계산한 gradient를 가져와 같이 업데이트한다.
- t번째 update : momentum_rate * t-1 번째 update  - learning_rate * t 번째의 gradient

# RMSProp

- 학습할수록 Learning rate를 줄여 나가며 학습하는 방법이다.
- 지수이동평균을 적용하여 과거일수록 gradient는 적게반영 후 learning rate를 조절한다.

# Adam

- Momentum과 RMSProp를 합친 방법이다.

