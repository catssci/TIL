# Sigmoid

- 활성화 함수 중 하나로 0과 1 사이의 실수를 출력하는 비선형 함수이다.
- 수식은 다음과 같이 사용한다.
  - $\frac{1}{1-e^{f(x)}}$

- 관련있는 모델로 **Logistic Regression, LSTM,GRU **이 있다.

- 관련 그림

  ![img](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1920px-Logistic-curve.svg.png)

# Sigmoid의 단점

- 특정구간에서 gradinet 값은 1에 가깝고, 이외에서는 gradient가 계속 작아진다.
- 그래서 모델 학습 시 **Gradient Vanishing** 문제가 발생 할 수 있다.

# Logistic Regression에서의 Sigmoid

- Binary Classification 문제에서는 Sigmoid를 사용하여 해당 출력이 클래스의 확률을 대표한다 할 수 있다.
- Multu Class Classification 문제에서는 Sigmoid와 Softmax를 사용하여 클래스의 확률을 벡터로 표현한다.
- 즉, Sigmoid의 결과는 하나의 클래스에 주목하여 클래스의 확률을 계산한다. 다른 클래스와의 관계를 표현하지 않는다.

# 미분 결과

- 모델 학습시에 활성화 함수의 미분값을 사용해야 한다.
- Sigmoid는 다음과 같이 미분을 할 수 있다.
  - $\frac{d}{dx}sigmoid(x) = sigmoid(x)*(1 - sigmoid(x))$

![sigmoid](https://machinelearningmastery.com/wp-content/uploads/2021/08/sigmoid.png)

- 미분 결과 최대값이 0.25이다. 즉, 모델이 깊어질수록 Sigmoid를 계속사용한다면 Gradient가 지속적으로 작아질 것이다.



# Reference

- https://machinelearningmastery.com/a-gentle-introduction-to-sigmoid-function/
